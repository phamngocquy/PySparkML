{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.2\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.6 (default, Sep 12 2018 18:26:19)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\"\n",
    "from pyspark.sql import *\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.shell import sc,spark\n",
    "from pyspark.sql.functions import desc,asc,expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[adult: string, belongs_to_collection: string, budget: string, genres: string, homepage: string, id: int, imdb_id: string, original_language: string, original_title: string, overview: string, popularity: string, poster_path: string, production_companies: string, production_countries: string, release_date: string, revenue: string, runtime: string, spoken_languages: string, status: string, tagline: string, title: string, video: string, vote_average: string, vote_count: string]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md = spark.read.csv(\"the-movies-dataset/movies_metadata.csv\",header=True)\n",
    "md = md.withColumn(\"id\",expr(\"CAST(id AS INTEGER)\"))\n",
    "print(len(md.collect()))\n",
    "md.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "lm = spark.read.csv(\"the-movies-dataset/links_small.csv\",header=True)\n",
    "lm = lm.withColumn('tmdbId',expr(\"CAST(tmdbId AS INTEGER)\"))\n",
    "joinExpression = lm[\"tmdbId\"] == md[\"id\"]\n",
    "smd = md.join(lm,joinExpression,\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col , column,when,lit,udf,split\n",
    "mergeCols = udf(lambda fruits, meat: fruits + meat)\n",
    "charReplace = udf(lambda x: x.replace(u',',''))\n",
    "\n",
    "# remove null value\n",
    "smd = smd.withColumn('overview', when(smd.tagline.isNull(),'').otherwise(smd['overview']))\n",
    "# smd = smd.withColumn('description',mergeCols(col('overview'),col('tagline')))\n",
    "# smd = smd.withColumn('description',charReplace(col('description')))\n",
    "smd = smd.withColumn(\"overview\",split(col(\"overview\"),\" \").cast(\"array<string>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               words|\n",
      "+--------------------+\n",
      "|   (319,[185],[1.0])|\n",
      "|(319,[0,1,2,3,5,9...|\n",
      "|(319,[0,1,2,5,6,7...|\n",
      "|(319,[0,1,2,3,4,1...|\n",
      "|(319,[0,1,2,3,4,5...|\n",
      "|(319,[0,1,2,4,7,1...|\n",
      "|(319,[1,3,8,13,17...|\n",
      "|(319,[0,1,2,3,4,5...|\n",
      "|(319,[0,1,2,3,4,5...|\n",
      "|(319,[0,2,3,4,15,...|\n",
      "+--------------------+\n",
      "\n",
      "[Row(features=SparseVector(319, {185: 1.7047})), Row(features=SparseVector(319, {0: 0.602, 1: 0.2007, 2: 0.4013, 3: 0.6369, 5: 0.6061, 9: 2.5986, 10: 2.0232, 11: 1.2993, 12: 0.7885, 13: 2.0232, 15: 1.0116, 19: 2.5986, 23: 1.2993, 25: 1.2993, 35: 1.2993, 45: 1.7047, 51: 1.7047, 53: 1.7047, 54: 1.7047, 60: 1.7047, 63: 1.7047, 67: 1.7047, 74: 1.7047, 80: 1.7047, 83: 1.7047, 87: 1.7047, 88: 1.7047, 89: 1.7047, 90: 1.7047, 102: 1.7047, 106: 1.7047, 109: 1.7047, 117: 1.7047, 118: 1.7047, 122: 1.7047, 129: 1.7047, 134: 1.7047, 136: 1.7047, 153: 1.7047, 154: 1.7047, 186: 1.7047, 188: 1.7047, 191: 1.7047, 198: 1.7047, 199: 1.7047, 201: 1.7047, 203: 1.7047, 219: 1.7047, 222: 1.7047, 223: 1.7047, 240: 1.7047, 242: 1.7047, 248: 1.7047, 251: 1.7047, 254: 1.7047, 258: 1.7047, 269: 1.7047, 301: 1.7047, 315: 1.7047})), Row(features=SparseVector(319, {0: 0.8027, 1: 0.602, 2: 0.4013, 5: 0.6061, 6: 0.7885, 7: 2.0232, 21: 1.2993, 23: 1.2993, 24: 3.4095, 29: 1.2993, 37: 1.2993, 42: 1.7047, 64: 1.7047, 70: 1.7047, 71: 1.7047, 78: 1.7047, 92: 1.7047, 101: 1.7047, 105: 1.7047, 115: 1.7047, 126: 1.7047, 145: 1.7047, 147: 1.7047, 151: 1.7047, 160: 1.7047, 164: 1.7047, 169: 1.7047, 174: 1.7047, 175: 1.7047, 183: 1.7047, 190: 1.7047, 227: 1.7047, 228: 1.7047, 229: 1.7047, 241: 1.7047, 256: 1.7047, 257: 1.7047, 263: 1.7047, 267: 1.7047, 268: 1.7047, 270: 1.7047, 272: 1.7047, 282: 1.7047, 292: 1.7047, 295: 1.7047, 299: 1.7047, 309: 1.7047, 312: 1.7047})), Row(features=SparseVector(319, {0: 0.4013, 1: 0.2007, 2: 0.4013, 3: 0.3185, 4: 0.452, 12: 0.7885, 13: 1.0116, 31: 1.2993, 36: 3.4095, 38: 1.7047, 58: 1.7047, 61: 1.7047, 79: 1.7047, 98: 1.7047, 103: 1.7047, 146: 1.7047, 150: 1.7047, 202: 1.7047, 204: 1.7047, 249: 1.7047, 274: 1.7047, 276: 1.7047, 278: 1.7047, 280: 1.7047, 285: 1.7047, 290: 1.7047})), Row(features=SparseVector(319, {0: 0.4013, 1: 0.602, 2: 0.4013, 3: 0.3185, 4: 0.904, 5: 0.6061, 6: 0.7885, 8: 2.0232, 9: 2.5986, 11: 3.8978, 12: 0.7885, 15: 1.0116, 16: 1.0116, 17: 1.0116, 20: 1.2993, 21: 1.2993, 26: 3.4095, 34: 1.2993, 48: 1.7047, 57: 1.7047, 62: 1.7047, 72: 1.7047, 77: 1.7047, 85: 1.7047, 93: 1.7047, 96: 1.7047, 130: 1.7047, 144: 1.7047, 152: 1.7047, 157: 1.7047, 181: 1.7047, 184: 1.7047, 205: 1.7047, 210: 1.7047, 211: 1.7047, 216: 1.7047, 232: 1.7047, 239: 1.7047, 243: 1.7047, 246: 1.7047, 259: 1.7047, 284: 1.7047, 288: 1.7047, 294: 1.7047, 296: 1.7047, 300: 1.7047, 311: 1.7047, 313: 1.7047, 317: 1.7047})), Row(features=SparseVector(319, {0: 0.602, 1: 0.4013, 2: 0.4013, 4: 0.452, 7: 1.0116, 12: 0.7885, 16: 1.0116, 19: 1.2993, 25: 1.2993, 31: 1.2993, 35: 1.2993, 41: 1.7047, 50: 1.7047, 66: 1.7047, 82: 1.7047, 94: 1.7047, 95: 1.7047, 97: 1.7047, 108: 1.7047, 112: 1.7047, 124: 1.7047, 133: 1.7047, 137: 1.7047, 140: 1.7047, 143: 1.7047, 149: 1.7047, 162: 1.7047, 171: 1.7047, 179: 1.7047, 180: 1.7047, 182: 1.7047, 189: 1.7047, 196: 1.7047, 209: 1.7047, 213: 1.7047, 214: 1.7047, 217: 1.7047, 225: 1.7047, 230: 1.7047, 233: 1.7047, 234: 1.7047, 252: 1.7047, 260: 1.7047, 261: 1.7047, 286: 1.7047, 302: 1.7047, 307: 1.7047, 314: 1.7047})), Row(features=SparseVector(319, {1: 0.4013, 3: 0.3185, 8: 1.0116, 13: 1.0116, 17: 1.0116, 20: 1.2993, 22: 1.2993, 46: 1.7047, 55: 1.7047, 56: 1.7047, 59: 1.7047, 110: 1.7047, 113: 1.7047, 119: 1.7047, 123: 1.7047, 127: 1.7047, 148: 1.7047, 156: 1.7047, 161: 1.7047, 163: 1.7047, 166: 1.7047, 192: 1.7047, 197: 1.7047, 212: 1.7047, 220: 1.7047, 221: 1.7047, 235: 1.7047, 255: 1.7047})), Row(features=SparseVector(319, {0: 0.602, 1: 0.602, 2: 0.4013, 3: 0.6369, 4: 0.452, 5: 0.6061, 6: 1.5769, 10: 1.0116, 14: 6.819, 17: 1.0116, 28: 3.4095, 29: 1.2993, 32: 3.4095, 33: 1.2993, 37: 1.2993, 40: 1.7047, 43: 1.7047, 44: 1.7047, 65: 1.7047, 68: 1.7047, 75: 1.7047, 104: 1.7047, 114: 1.7047, 116: 1.7047, 131: 1.7047, 132: 1.7047, 135: 1.7047, 141: 1.7047, 165: 1.7047, 173: 1.7047, 193: 1.7047, 194: 1.7047, 195: 1.7047, 206: 1.7047, 207: 1.7047, 226: 1.7047, 236: 1.7047, 245: 1.7047, 264: 1.7047, 279: 1.7047, 287: 1.7047, 291: 1.7047, 293: 1.7047, 298: 1.7047, 305: 1.7047, 306: 1.7047, 310: 1.7047, 318: 1.7047})), Row(features=SparseVector(319, {0: 0.602, 1: 1.204, 2: 0.2007, 3: 0.3185, 4: 0.452, 5: 0.6061, 6: 0.7885, 7: 2.0232, 8: 1.0116, 10: 1.0116, 18: 5.1142, 22: 1.2993, 27: 3.4095, 30: 3.4095, 33: 1.2993, 34: 1.2993, 39: 1.7047, 47: 1.7047, 49: 1.7047, 52: 1.7047, 69: 1.7047, 76: 1.7047, 81: 1.7047, 84: 1.7047, 100: 1.7047, 107: 1.7047, 111: 1.7047, 128: 1.7047, 138: 1.7047, 139: 1.7047, 142: 1.7047, 155: 1.7047, 158: 1.7047, 167: 1.7047, 168: 1.7047, 170: 1.7047, 172: 1.7047, 176: 1.7047, 178: 1.7047, 200: 1.7047, 208: 1.7047, 218: 1.7047, 237: 1.7047, 247: 1.7047, 250: 1.7047, 262: 1.7047, 265: 1.7047, 266: 1.7047, 271: 1.7047, 273: 1.7047, 275: 1.7047, 277: 1.7047, 281: 1.7047, 283: 1.7047, 297: 1.7047, 303: 1.7047, 304: 1.7047, 308: 1.7047})), Row(features=SparseVector(319, {0: 0.8027, 2: 0.2007, 3: 0.3185, 4: 0.452, 15: 1.0116, 16: 1.0116, 73: 1.7047, 86: 1.7047, 91: 1.7047, 99: 1.7047, 120: 1.7047, 121: 1.7047, 125: 1.7047, 159: 1.7047, 177: 1.7047, 187: 1.7047, 215: 1.7047, 224: 1.7047, 231: 1.7047, 238: 1.7047, 244: 1.7047, 253: 1.7047, 289: 1.7047, 316: 1.7047}))]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "smd.printSchema\n",
    "tf = CountVectorizer(inputCol=\"overview\",outputCol=\"words\")\n",
    "model =  tf.fit(smd.limit(10))\n",
    "result = model.transform(smd.limit(10))\n",
    "result.select('words').show(10,truncate=True)\n",
    "# smd.printSchema\n",
    "idf = IDF(inputCol=\"words\", outputCol=\"features\")\n",
    "idfModel = idf.fit(result)\n",
    "rescaledData = idfModel.transform(result)\n",
    "\n",
    "print(rescaledData.select(\"features\").collect())\n",
    "# tokenizer = Tokenizer(inputCol=\"description\",outputCol='words')\n",
    "# wordsdata = tokenizer.transform(smd)\n",
    "# hashingTF = HashingTF(inputCol='words',outputCol='rawFeatures',numFeatures=100)\n",
    "# featurizedData = hashingTF.transform(wordsdata)\n",
    "# featurizedData.select('rawFeatures').show(1,False)\n",
    "\n",
    "\n",
    "# idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "# idfModel = idf.fit(featurizedData)\n",
    "# rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "# tmp = rescaledData.select(\"label\", \"features\").collect()\n",
    "# rescaledData.select(\"label\", \"features\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------+------------------------------------------+\n",
      "|label|sentence                           |words                                     |\n",
      "+-----+-----------------------------------+------------------------------------------+\n",
      "|0.0  |Hi I heard about Spark             |[hi, i, heard, about, spark]              |\n",
      "|0.0  |I wish Java could use case classes |[i, wish, java, could, use, case, classes]|\n",
      "|1.0  |Logistic regression models are neat|[logistic, regression, models, are, neat] |\n",
      "+-----+-----------------------------------+------------------------------------------+\n",
      "\n",
      "+---------------------------------------------------------+\n",
      "|rawFeatures                                              |\n",
      "+---------------------------------------------------------+\n",
      "|(100,[5,29,57,60,77],[1.0,1.0,1.0,1.0,1.0])              |\n",
      "|(100,[9,13,29,42,67,89,95],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(100,[4,38,86,93,95],[1.0,1.0,1.0,1.0,1.0])              |\n",
      "+---------------------------------------------------------+\n",
      "\n",
      "+-----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                                                                                                                            |\n",
      "+-----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0  |(100,[5,29,57,60,77],[0.6931471805599453,0.28768207245178085,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                             |\n",
      "|0.0  |(100,[9,13,29,42,67,89,95],[0.6931471805599453,0.6931471805599453,0.28768207245178085,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.28768207245178085])|\n",
      "|1.0  |(100,[4,38,86,93,95],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.28768207245178085])                                             |\n",
      "+-----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "wordsData.show(10,False)\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=100)\n",
    "\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "featurizedData.select('rawFeatures').show(10,False)\n",
    "# # alternatively, CountVetorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "tmp = rescaledData.select(\"label\", \"features\").collect()\n",
    "rescaledData.select(\"label\", \"features\").show(10,False)\n",
    "# print(tmp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "\n",
    "# Load documents (one per line).\n",
    "documents = sc.textFile(\"data/mllib/kmeans_data.txt\").map(lambda line: line.split(\" \"))\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(documents)\n",
    "\n",
    "# While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "# First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "# spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "# which occur in less than a minimum number of documents.\n",
    "# In such cases, the IDF for these terms is set to 0.\n",
    "# This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "idfIgnore = IDF(minDocFreq=2).fit(tf)\n",
    "tfidfIgnore = idfIgnore.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+\n",
      "|text                                      |\n",
      "+------------------------------------------+\n",
      "|[Hi, I, heard, about, Spark]              |\n",
      "|[I, wish, Java, could, use, case, classes]|\n",
      "|[Logistic, regression, models, are, neat] |\n",
      "+------------------------------------------+\n",
      "\n",
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [-0.0190316129475832,-0.0027922827750444415,0.008608809486031533]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [0.05155504494905472,-0.028778317916606153,0.021439425381166593]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [-0.002297615259885788,0.05957087948918343,0.05615671277046204]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "documentDF.show(10,False)\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    words|\n",
      "+---------+\n",
      "|    a b c|\n",
      "|a b b c a|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Column words must be of type equal to one of the following types: [ArrayType(StringType,true), ArrayType(StringType,false)] but was actually of type StringType.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o598.fit.\n: java.lang.IllegalArgumentException: requirement failed: Column words must be of type equal to one of the following types: [ArrayType(StringType,true), ArrayType(StringType,false)] but was actually of type StringType.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkColumnTypes(SchemaUtils.scala:58)\n\tat org.apache.spark.ml.feature.CountVectorizerParams$class.validateAndTransformSchema(CountVectorizer.scala:75)\n\tat org.apache.spark.ml.feature.CountVectorizer.validateAndTransformSchema(CountVectorizer.scala:123)\n\tat org.apache.spark.ml.feature.CountVectorizer.transformSchema(CountVectorizer.scala:188)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.CountVectorizer.fit(CountVectorizer.scala:155)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8d50d1c527e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"words\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \"\"\"\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: Column words must be of type equal to one of the following types: [ArrayType(StringType,true), ArrayType(StringType,false)] but was actually of type StringType.'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Input data: Each row is a bag of words with a ID.\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"a b c\".split(\" \")),\n",
    "    (1, \"a b b c a\".split(\" \"))\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "df.select(\"words\").show()\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "\n",
    "model = cv.fit(df)\n",
    "\n",
    "result = model.transform(df)\n",
    "result.show(truncate=False)\n",
    "df.printSchema"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
