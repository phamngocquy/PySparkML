{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.2\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.6 (default, Sep 12 2018 18:26:19)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\"\n",
    "import pandas as pd\n",
    "from pyspark.sql import *\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.shell import sc,spark\n",
    "from pyspark.sql.functions import desc,asc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|     31|   2.5|1260759144|\n",
      "|     1|   1029|   3.0|1260759179|\n",
      "|     1|   1061|   3.0|1260759182|\n",
      "|     1|   1129|   2.0|1260759185|\n",
      "|     1|   1172|   4.0|1260759205|\n",
      "|     1|   1263|   2.0|1260759151|\n",
      "|     1|   1287|   2.0|1260759187|\n",
      "|     1|   1293|   2.0|1260759148|\n",
      "|     1|   1339|   3.5|1260759125|\n",
      "|     1|   1343|   2.0|1260759131|\n",
      "|     1|   1371|   2.5|1260759135|\n",
      "|     1|   1405|   1.0|1260759203|\n",
      "|     1|   1953|   4.0|1260759191|\n",
      "|     1|   2105|   4.0|1260759139|\n",
      "|     1|   2150|   3.0|1260759194|\n",
      "|     1|   2193|   2.0|1260759198|\n",
      "|     1|   2294|   2.0|1260759108|\n",
      "|     1|   2455|   2.5|1260759113|\n",
      "|     1|   2968|   1.0|1260759200|\n",
      "|     1|   3671|   3.0|1260759117|\n",
      "+------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[userId: string, movieId: string, rating: string, timestamp: string]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv(\"the-movies-dataset/ratings_small.csv\",header=True)\n",
    "df.show()\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|   2105|   4.0|\n",
      "|     1|   1953|   4.0|\n",
      "|     1|   1172|   4.0|\n",
      "|     1|   1339|   3.5|\n",
      "|     1|   1029|   3.0|\n",
      "|     1|   1061|   3.0|\n",
      "|     1|   2150|   3.0|\n",
      "|     1|   3671|   3.0|\n",
      "|     1|     31|   2.5|\n",
      "|     1|   1371|   2.5|\n",
      "|     1|   2455|   2.5|\n",
      "|     1|   1263|   2.0|\n",
      "|     1|   1343|   2.0|\n",
      "|     1|   1129|   2.0|\n",
      "|     1|   1287|   2.0|\n",
      "|     1|   2193|   2.0|\n",
      "|     1|   1293|   2.0|\n",
      "|     1|   2294|   2.0|\n",
      "|     1|   1405|   1.0|\n",
      "|     1|   2968|   1.0|\n",
      "+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "rate = df.select(\"userId\",\"movieId\",\"rating\")\n",
    "rate = rate.withColumn(\"userId\", expr(\"CAST(userId AS INTEGER)\")).withColumn(\"movieId\", expr(\n",
    "        \"CAST(movieId AS INTEGER)\")).withColumn(\"rating\", expr(\"CAST(rating AS FLOAT)\"))\n",
    "rate.filter(\"userId = 1\").sort(desc('rating')).show()\n",
    "training, test = rate.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: alpha for implicit preference (default: 1.0)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "coldStartStrategy: strategy for dealing with unknown or new users/items at prediction time. This may be useful in cross-validation or production scenarios, for handling user/item ids the model has not seen in the training data. Supported values: 'nan', 'drop'. (default: nan, current: drop)\n",
      "finalStorageLevel: StorageLevel for ALS model factors. (default: MEMORY_AND_DISK)\n",
      "implicitPrefs: whether to use implicit preference (default: False)\n",
      "intermediateStorageLevel: StorageLevel for intermediate datasets. Cannot be 'NONE'. (default: MEMORY_AND_DISK)\n",
      "itemCol: column name for item ids. Ids must be within the integer value range. (default: item, current: movieId)\n",
      "maxIter: max number of iterations (>= 0). (default: 10, current: 19)\n",
      "nonnegative: whether to use nonnegative constraint for least squares (default: False)\n",
      "numItemBlocks: number of item blocks (default: 10)\n",
      "numUserBlocks: number of user blocks (default: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "rank: rank of the factorization (default: 10, current: 10)\n",
      "ratingCol: column name for ratings (default: rating, current: rating)\n",
      "regParam: regularization parameter (>= 0). (default: 0.1, current: 0.18)\n",
      "seed: random seed. (default: 7728828318469785859)\n",
      "userCol: column name for user ids. Ids must be within the integer value range. (default: user, current: userId)\n",
      "+------+-------+---------+\n",
      "|userId|movieId|   rating|\n",
      "+------+-------+---------+\n",
      "|     1|   1172|      4.0|\n",
      "|     1|   1953|      4.0|\n",
      "|     1|   2105|      4.0|\n",
      "|     1|  83318|3.5577128|\n",
      "|     1|  83411|3.5577128|\n",
      "|     1|  67504|3.5577128|\n",
      "|     1|  83359|3.5577128|\n",
      "|     1|   1339|      3.5|\n",
      "|     1|    116|3.2630439|\n",
      "|     1|  54328|3.2215323|\n",
      "|     1|   7074|3.2019415|\n",
      "|     1|   4405|3.2019415|\n",
      "|     1|  25764|3.2019415|\n",
      "|     1|  26400|3.2019415|\n",
      "|     1|   1029|      3.0|\n",
      "|     1|   1061|      3.0|\n",
      "|     1|   2150|      3.0|\n",
      "|     1|   3671|      3.0|\n",
      "|     1|     31|      2.5|\n",
      "|     1|   1371|      2.5|\n",
      "|     1|   2455|      2.5|\n",
      "|     1|   1129|      2.0|\n",
      "|     1|   1263|      2.0|\n",
      "|     1|   1287|      2.0|\n",
      "|     1|   1293|      2.0|\n",
      "|     1|   1343|      2.0|\n",
      "|     1|   2193|      2.0|\n",
      "|     1|   2294|      2.0|\n",
      "|     1|   1405|      1.0|\n",
      "|     1|   2968|      1.0|\n",
      "+------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "# ratings = sc.parallelize(ratings)\n",
    "# model = ALS.train(training,10,10)\n",
    "# model.recommendProducts(1,10)\n",
    "\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "als = ALS().setMaxIter(19).setRegParam(0.18).setRank(10).setColdStartStrategy(\"drop\").setUserCol(\"userId\").setItemCol(\"movieId\").setRatingCol(\"rating\")\n",
    "alsModel = als.fit(training)\n",
    "print(als.explainParams())\n",
    "\n",
    "result = alsModel.recommendForAllUsers(10).selectExpr(\"userId\", \"explode(recommendations) as recommendations \")\n",
    "tmp = result.select(\"userId\",\"recommendations.movieId\",\"recommendations.rating\").filter(\"userId = 1\")\n",
    "# tmp = result.filter('userId = 1').sort(desc('recommendations.rating'))\n",
    "rate = rate.filter(\"userId = 1\").union(tmp)\n",
    "rate = rate.sort(desc('rating')).distinct().show(40)\n",
    "# print(type(rate))\n",
    "# print(len(rate.collect()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 0.905108\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator().setMetricName(\"rmse\").setLabelCol(\"rating\").setPredictionCol(\"prediction\")\n",
    "predictions = alsModel.transform(test)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = %f\"%rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(number + 10)|\n",
      "+-------------+\n",
      "|           10|\n",
      "|           11|\n",
      "|           12|\n",
      "|           13|\n",
      "|           14|\n",
      "|           15|\n",
      "|           16|\n",
      "|           17|\n",
      "|           18|\n",
      "|           19|\n",
      "|           20|\n",
      "|           21|\n",
      "|           22|\n",
      "|           23|\n",
      "|           24|\n",
      "|           25|\n",
      "|           26|\n",
      "|           27|\n",
      "|           28|\n",
      "|           29|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(500).toDF(\"number\")\n",
    "tmp = df.select(df[\"number\"]+10)\n",
    "tmp.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
